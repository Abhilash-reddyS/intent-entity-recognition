{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd4bb38",
   "metadata": {},
   "source": [
    "**Disclaimer :**\n",
    " Parts of the code were generated with the assistance of OpenAI’s GPT-5 model. I have reviewed and adapted the outputs for correctness and relevance to this work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d790d28",
   "metadata": {},
   "source": [
    "# Task 1 — Wizard of Tasks: Intent Recognition\n",
    "\n",
    "\n",
    "This notebook trains **three models** on Wizard of Tasks (≈18k utterances):\n",
    "- Multinomial **Naïve Bayes** (NB)\n",
    "- **Logistic Regression** (Linear BoW / Softmax)\n",
    "- **MLP** (simple neural baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba483986",
   "metadata": {},
   "source": [
    "## 1) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c0e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re, time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344402b1",
   "metadata": {},
   "source": [
    "## 2) Load Wizard of Tasks JSON (flexible schema)\n",
    "\n",
    "Supports both schemas:\n",
    "- **Schema A**: dict → `data_split`, `turns` with `role`, `text`, `intent`.\n",
    "- **Schema B**: list → `split`, `dialog` with `speaker`, `text`, `intent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70afc8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_COOK = Path(\"./wizard_of_tasks_cooking_v1.0.json\")\n",
    "DATA_DIY  = Path(\"./wizard_of_tasks_diy_v1.0.json\")\n",
    "\n",
    "def _extract_from_schema_a(obj: Dict, domain: str):\n",
    "    rows = []\n",
    "    for conv_id, conv in obj.items():\n",
    "        split = conv.get(\"data_split\") or conv.get(\"split\") or \"train\"\n",
    "        for turn in conv.get(\"turns\", []):\n",
    "            role = turn.get(\"role\")\n",
    "            text = (turn.get(\"text\") or \"\").strip()\n",
    "            intent = turn.get(\"intent\")\n",
    "            if role == \"student\" and text and intent:\n",
    "                rows.append({\"text\": text, \"intent\": intent, \"split\": split, \"domain\": domain, \"conv_id\": conv_id})\n",
    "    return rows\n",
    "\n",
    "def _extract_from_schema_b(lst: List, domain: str):\n",
    "    rows = []\n",
    "    for conv in lst:\n",
    "        split = conv.get(\"split\") or conv.get(\"data_split\") or \"train\"\n",
    "        conv_id = conv.get(\"id\") or conv.get(\"conversation_id\") or None\n",
    "        for turn in conv.get(\"dialog\", []):\n",
    "            speaker = turn.get(\"speaker\") or turn.get(\"role\")\n",
    "            text = (turn.get(\"text\") or \"\").strip()\n",
    "            intent = turn.get(\"intent\")\n",
    "            if (speaker == \"student\" or speaker == \"user\") and text and intent:\n",
    "                rows.append({\"text\": text, \"intent\": intent, \"split\": split, \"domain\": domain, \"conv_id\": conv_id})\n",
    "    return rows\n",
    "\n",
    "def load_wot(path: Path, domain: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    if isinstance(data, dict):\n",
    "        return _extract_from_schema_a(data, domain)\n",
    "    elif isinstance(data, list):\n",
    "        return _extract_from_schema_b(data, domain)\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized JSON structure.\")\n",
    "\n",
    "rows = []\n",
    "for pth, dom in [(DATA_COOK, \"cooking\"), (DATA_DIY, \"diy\")]:\n",
    "    if not pth.exists():\n",
    "        raise FileNotFoundError(f\"Missing file: {pth.resolve()}\")\n",
    "    rows += load_wot(pth, dom)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"Total student turns with intents:\", len(df))\n",
    "print(\"Unique intents:\", df['intent'].nunique())\n",
    "display(df.head())\n",
    "print(\"\\nSplit counts:\")\n",
    "print(df['split'].value_counts())\n",
    "print(\"\\nDomain counts:\")\n",
    "print(df['domain'].value_counts())\n",
    "# print(\"Unique intents:\", df['intent'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976a4f64",
   "metadata": {},
   "source": [
    "## 3) Preprocessing & Vectorizers\n",
    "\n",
    "- Lowercasing\n",
    "- **Negation handling**: `not good` → `NOT_good`\n",
    "- Unigrams + bigrams\n",
    "- **Count** (NB) and **TF‑IDF** (LR/MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dac35fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEGATORS = {\"no\",\"not\",\"never\",\"none\",\"n't\"}\n",
    "\n",
    "def negate_bigram_tokens(text: str) -> str:\n",
    "    toks = re.findall(r\"[A-Za-z']+|\\d+|[^\\w\\s]\", text.lower())\n",
    "    out, negate = [], False\n",
    "    for tok in toks:\n",
    "        if tok in NEGATORS:\n",
    "            negate = True; out.append(tok); continue\n",
    "        if re.match(r\"\\w+\", tok) and negate:\n",
    "            out.append(f\"NOT_{tok}\"); negate=False\n",
    "        else:\n",
    "            out.append(tok)\n",
    "    return \" \".join(out)\n",
    "\n",
    "def preproc(text: str) -> str:\n",
    "    return negate_bigram_tokens(text)\n",
    "\n",
    "count_vec = CountVectorizer(ngram_range=(1,2), min_df=5, preprocessor=preproc, binary=True)\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,2), min_df=5, preprocessor=preproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944345a2",
   "metadata": {},
   "source": [
    "## 4) Train/Test Split (prefer official split; else stratified 80/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a5ff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_official = df['split'].str.lower().isin(['train','test','dev','validation']).any()\n",
    "if has_official:\n",
    "    train_mask = df['split'].str.lower().isin(['train','trn'])\n",
    "    test_mask  = df['split'].str.lower().isin(['test','tst'])\n",
    "    if not test_mask.any():\n",
    "        non_train = df.loc[~train_mask]\n",
    "        tr_df, te_df = train_test_split(non_train, test_size=0.5, stratify=non_train['intent'], random_state=42)\n",
    "        train_df = pd.concat([df.loc[train_mask], tr_df], ignore_index=True)\n",
    "        test_df = te_df.copy()\n",
    "    else:\n",
    "        train_df = df.loc[train_mask].copy()\n",
    "        test_df  = df.loc[test_mask].copy()\n",
    "else:\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['intent'], random_state=42)\n",
    "\n",
    "print(\"Train size:\", len(train_df), \"| Test size:\", len(test_df))\n",
    "X_train, y_train = train_df['text'].tolist(), train_df['intent'].tolist()\n",
    "X_test,  y_test  = test_df['text'].tolist(),  test_df['intent'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d17ab5",
   "metadata": {},
   "source": [
    "## 5) Models (NB, LR, MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3355294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeit_fit_predict(pipe, Xtr, ytr, Xte):\n",
    "    t0=time.time(); pipe.fit(Xtr, ytr); t1=time.time()\n",
    "    yhat = pipe.predict(Xte); t2=time.time()\n",
    "    return {\"train_time_s\": t1-t0, \"infer_time_s\": t2-t1, \"y_pred\": yhat, \"pipe\": pipe}\n",
    "\n",
    "nb_pipe = Pipeline([(\"vec\", count_vec), (\"clf\", MultinomialNB())])\n",
    "lr_pipe = Pipeline([(\"vec\", tfidf_vec), (\"clf\", LogisticRegression(max_iter=2000, n_jobs=-1))])\n",
    "mlp_pipe = Pipeline([\n",
    "    (\"vec\", tfidf_vec),\n",
    "    (\"clf\", MLPClassifier(\n",
    "        hidden_layer_sizes=(256,),\n",
    "        activation=\"relu\",\n",
    "        batch_size=64,\n",
    "        max_iter=20,          # a few more iters since no early stopping\n",
    "        early_stopping=False, # <-- key change\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "results = {}\n",
    "for name, pipe in [(\"NaiveBayes\", nb_pipe), (\"LogReg\", lr_pipe), (\"MLP\", mlp_pipe)]:\n",
    "    print(f\"\\nTraining {name} ...\")\n",
    "    out = timeit_fit_predict(pipe, X_train, y_train, X_test)\n",
    "    acc = accuracy_score(y_test, out[\"y_pred\"])\n",
    "    f1_micro = f1_score(y_test, out[\"y_pred\"], average=\"micro\")\n",
    "    f1_macro = f1_score(y_test, out[\"y_pred\"], average=\"macro\")\n",
    "    results[name] = {**out, \"accuracy\": acc, \"f1_micro\": f1_micro, \"f1_macro\": f1_macro}\n",
    "    print(f\"{name}: ACC={acc:.3f} | F1_micro={f1_micro:.3f} | F1_macro={f1_macro:.3f} | train {out['train_time_s']:.2f}s | infer {out['infer_time_s']:.2f}s\")\n",
    "\n",
    "pd.DataFrame({k:{m:v[m] for m in ['accuracy','f1_micro','f1_macro','train_time_s','infer_time_s']} for k,v in results.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bb6d5c",
   "metadata": {},
   "source": [
    "## 6) Detailed Evaluation (best by Macro F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c069032",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_name = max(results, key=lambda k: results[k]['f1_macro'])\n",
    "best = results[best_name]\n",
    "print(\"Best model:\", best_name)\n",
    "print(classification_report(y_test, best['y_pred'], digits=3))\n",
    "\n",
    "labels = sorted(np.unique(y_test + list(best['y_pred'])))\n",
    "cm = confusion_matrix(y_test, best['y_pred'], labels=labels)\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(cm, annot=False, cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "plt.title(f\"Confusion Matrix — {best_name}\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a89c575",
   "metadata": {},
   "source": [
    "## 7) Interpretability (LogReg): Top n‑grams per intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735b3bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_features_logreg(pipe, n=12):\n",
    "    if not isinstance(pipe.named_steps['clf'], LogisticRegression):\n",
    "        print(\"Top features only for LogisticRegression\"); return None\n",
    "    vec = pipe.named_steps['vec']; clf = pipe.named_steps['clf']\n",
    "    feats = np.array(vec.get_feature_names_out())\n",
    "    coefs = clf.coef_; classes = clf.classes_\n",
    "    out = {}\n",
    "    for i, cls in enumerate(classes):\n",
    "        idx = np.argsort(coefs[i])[::-1][:n]\n",
    "        out[cls] = list(zip(feats[idx], coefs[i][idx]))\n",
    "    return out\n",
    "\n",
    "if 'LogReg' in results:\n",
    "    tops = top_features_logreg(results['LogReg']['pipe'], n=12)\n",
    "    if tops:\n",
    "        for cls, pairs in list(tops.items())[:8]:\n",
    "            print(f\"\\nIntent: {cls}\")\n",
    "            for f,w in pairs: print(f\"  {f:35s} {w:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ba9c38",
   "metadata": {},
   "source": [
    "## 8) Domain-wise Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535e23e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'domain' in test_df.columns and len(test_df['domain'].unique())>1:\n",
    "    dom_scores = []\n",
    "    best_pipe = best['pipe']\n",
    "    for dom in sorted(test_df['domain'].unique()):\n",
    "        mask = (test_df['domain']==dom)\n",
    "        y_true = test_df.loc[mask, 'intent'].tolist()\n",
    "        y_pred = best_pipe.predict(test_df.loc[mask, 'text'].tolist())\n",
    "        dom_scores.append({\n",
    "            \"domain\": dom,\n",
    "            \"n\": int(mask.sum()),\n",
    "            \"acc\": accuracy_score(y_true, y_pred),\n",
    "            \"f1_micro\": f1_score(y_true, y_pred, average=\"micro\"),\n",
    "            \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        })\n",
    "    display(pd.DataFrame(dom_scores).sort_values(\"f1_macro\", ascending=False))\n",
    "else:\n",
    "    print(\"Domain info not available or only one domain in test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8b452a",
   "metadata": {},
   "source": [
    "## 9) Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72509b34",
   "metadata": {},
   "source": [
    "\n",
    "- **Model comparison:**  \n",
    "  Naïve Bayes and Logistic Regression both performed strongly, with accuracy ≈ 84%.  \n",
    "  • **Naïve Bayes** achieved the best **macro-F1 (0.54)**, showing better balance across all intent classes, including the rarer ones.  \n",
    "  • **Logistic Regression** had the best **overall accuracy (0.84)** and micro-F1, meaning it handled the frequent intents slightly better.  \n",
    "  • **MLP** underperformed (accuracy 0.80, macro-F1 0.52) and was much slower, indicating that for sparse bag-of-words data, a neural baseline is not competitive with linear models.\n",
    "\n",
    "- **Efficiency:**  \n",
    "  • Training time: NB was fastest (0.11 s), LR was moderate (≈ 1 s), and MLP was slowest (≈ 16 s).  \n",
    "  • Inference time: all models were very fast (< 0.02 s per test set), but NB and LR are clearly more efficient to train and deploy.  \n",
    "  • Memory footprint: NB and LR store only feature–class weights, while MLP requires thousands of extra parameters.\n",
    "\n",
    "- **Robustness:**  \n",
    "  • NB’s higher macro-F1 suggests it is more robust across rare intents, while LR tends to favor majority classes.  \n",
    "  • LR is likely to generalize better under domain shifts (cooking ↔ DIY), since it directly optimizes conditional likelihood.  \n",
    "  • MLP offered no extra robustness; it was more prone to overfitting without delivering higher scores.\n",
    "\n",
    "- **Feature analysis:**  \n",
    "  • Top LR features confirmed intuitive intent cues: question words (“how”, “what”, “do I”), cooking terms (“chop”, “oven”), and DIY tools (“nails”, “hammer”).  \n",
    "  • Negation handling (e.g., “NOT_ready”) helped separate confirm/deny intents.  \n",
    "  • Bigram features captured useful context (e.g., “next step”, “how long”), showing why linear BoW models work so well in this domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2638bf69",
   "metadata": {},
   "source": [
    "# Task 2 — Jericho Entity Recognition (BIO Tagger + Seq2Seq Targets)\n",
    "\n",
    "**Problem statement:**  \n",
    "Given a **location description** from a text-based world (Jericho/TextWorld), **predict the set of interactive objects**.  \n",
    "We implement **BIO tagging** (token-level) and also prepare data for an optional **Seq2Seq** model.\n",
    "\n",
    "**Why this matters for a pipeline agent:**  \n",
    "Extracted entities feed into a downstream action selector (e.g., build a knowledge graph like in Ammanabrolu et al. and choose actions such as *open mailbox*, *go north*, etc.).\n",
    "\n",
    "**Example :**  \n",
    "Input: “… behind the white house. A path leads into the forest …”  \n",
    "Output (BIO tagger): “… O O B I O B O O O O …” → objects: “white house”, “path”  \n",
    "Output (Seq2Seq): “…, white house, path, …”\n",
    "\n",
    "> Not all nouns are interactable (e.g., *forest* may be too far). We aim to predict the **interactable objects**, as given by the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c8be93",
   "metadata": {},
   "source": [
    "## 0) Environment check & installs (run once if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206d3d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q sklearn-crfsuite pandas numpy matplotlib seaborn tqdm\n",
    "import sys, platform\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3d368b",
   "metadata": {},
   "source": [
    "## 1) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e456d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re, random, math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Set, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics as crf_metrics\n",
    "\n",
    "random.seed(42); np.random.seed(42)\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eac6b9",
   "metadata": {},
   "source": [
    "## 2) Config — file paths & truncation controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb1f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = Path(\"./train.json\")\n",
    "TEST_PATH  = Path(\"./test.json\")\n",
    "\n",
    "# Truncation controls (set to None to disable)\n",
    "MAX_TRAIN_EXAMPLES = None   # e.g., 10000\n",
    "MAX_TEST_EXAMPLES  = None   # e.g., 2000\n",
    "MAX_TOKENS         = 256    # truncate long loc_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df7b734",
   "metadata": {},
   "source": [
    "## 3) Load & normalize Jericho data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863d49f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Dict\n",
    "\n",
    "def _norm_objects(objs_any: Any) -> List[str]:\n",
    "    if objs_any is None:\n",
    "        return []\n",
    "    if isinstance(objs_any, list):\n",
    "        out = []\n",
    "        for v in objs_any:\n",
    "            if isinstance(v, list):\n",
    "                out.append(\" \".join(map(str, v)))\n",
    "            else:\n",
    "                out.append(str(v))\n",
    "        return [s.strip() for s in out if s and s.strip()]\n",
    "    if isinstance(objs_any, dict):\n",
    "        out = []\n",
    "        for v in objs_any.values():\n",
    "            if isinstance(v, list):\n",
    "                out.append(\" \".join(map(str, v)))\n",
    "            else:\n",
    "                out.append(str(v))\n",
    "        return [s.strip() for s in out if s and s.strip()]\n",
    "    if isinstance(objs_any, str):\n",
    "        return [s.strip() for s in objs_any.split(\",\") if s.strip()]\n",
    "    return [str(objs_any).strip()] if str(objs_any).strip() else []\n",
    "\n",
    "def _extract_from_example(ex: Any) -> Dict[str, Any]:\n",
    "    if isinstance(ex, dict):\n",
    "        loc = ex.get(\"loc_desc\") or ex.get(\"location\") or ex.get(\"description\")\n",
    "        objs = ex.get(\"surrounding_objects\") or ex.get(\"objects\") or ex.get(\"interactive_objects\")\n",
    "        return {\"loc_desc\": str(loc) if loc is not None else \"\", \"surrounding_objects\": _norm_objects(objs)}\n",
    "    if isinstance(ex, (list, tuple)):\n",
    "        loc_candidates = [e for e in ex if isinstance(e, str)]\n",
    "        loc = max(loc_candidates, key=len) if loc_candidates else (str(ex[0]) if ex else \"\")\n",
    "        objs_candidate = None\n",
    "        for e in reversed(ex):\n",
    "            if isinstance(e, (list, dict, str)):\n",
    "                objs_candidate = e; break\n",
    "        objs = _norm_objects(objs_candidate)\n",
    "        return {\"loc_desc\": loc, \"surrounding_objects\": objs}\n",
    "    return {\"loc_desc\": str(ex), \"surrounding_objects\": []}\n",
    "\n",
    "def load_split(path: Path, max_examples=None) -> List[Dict[str, Any]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    if isinstance(data, dict):\n",
    "        for k in (\"data\",\"examples\",\"items\",\"records\"):\n",
    "            if isinstance(data.get(k), list):\n",
    "                data = data[k]; break\n",
    "    assert isinstance(data, list), f\"Top-level JSON must be a list; got {type(data)}\"\n",
    "    print(f\"Loaded {len(data)} items from {path.name}. Preview of first 2:\")\n",
    "    for i, ex in enumerate(data[:2]):\n",
    "        print(f\"  [{i}] type={type(ex).__name__} ->\",\n",
    "              (list(ex.keys()) if isinstance(ex, dict) else f\"len={len(ex)}\"))\n",
    "    out = [_extract_from_example(ex) for ex in data]\n",
    "    if max_examples is not None:\n",
    "        out = out[:max_examples]\n",
    "    return out\n",
    "\n",
    "train_data = load_split(TRAIN_PATH, MAX_TRAIN_EXAMPLES)\n",
    "test_data  = load_split(TEST_PATH,  MAX_TEST_EXAMPLES)\n",
    "print(\"Train examples (normalized):\", len(train_data))\n",
    "print(\"Test  examples (normalized):\", len(test_data))\n",
    "print(\"Sample normalized item:\", train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44a8482",
   "metadata": {},
   "source": [
    "## 4) Tokenization, normalization & truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfedc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_RE = re.compile(r\"[A-Za-z]+(?:'[A-Za-z]+)?|\\d+|[^\\w\\s]\")\n",
    "ARTICLES = {\"a\",\"an\",\"the\"}\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    return TOKEN_RE.findall(text)\n",
    "\n",
    "def normalize_tokens(ts: List[str]) -> List[str]:\n",
    "    out=[]\n",
    "    for t in ts:\n",
    "        tl=t.lower()\n",
    "        if tl in ARTICLES: \n",
    "            continue\n",
    "        if re.fullmatch(r\"\\W\", t): \n",
    "            continue\n",
    "        if tl.endswith(\"es\") and len(tl)>4: tl = tl[:-2]\n",
    "        elif tl.endswith(\"s\") and len(tl)>3: tl = tl[:-1]\n",
    "        out.append(tl)\n",
    "    return out\n",
    "\n",
    "def truncate_tokens(toks: List[str], max_len: int) -> List[str]:\n",
    "    if max_len is None: return toks\n",
    "    return toks[:max_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95199d6e",
   "metadata": {},
   "source": [
    "## 5) BIO label engineering (robust span alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98efddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "from typing import Tuple\n",
    "\n",
    "def windows(tokens: List[str], min_len=1, max_len=6):\n",
    "    for L in range(max_len, min_len-1, -1):\n",
    "        for i in range(0, len(tokens)-L+1):\n",
    "            yield i, i+L\n",
    "\n",
    "def fuzzy_ratio(a: str, b: str) -> float:\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def bio_label(loc_desc: str, object_phrases: List[str], max_tokens=256) -> Tuple[List[str], List[str]]:\n",
    "    toks_full = tokenize(loc_desc)\n",
    "    toks = truncate_tokens(toks_full, max_tokens)\n",
    "    labels = [\"O\"] * len(toks)\n",
    "    used = [False]*len(toks)\n",
    "\n",
    "    norm_windows = []\n",
    "    for s,e in windows(toks, min_len=1, max_len=6):\n",
    "        span_norm = \" \".join(normalize_tokens(toks[s:e]))\n",
    "        if span_norm:\n",
    "            norm_windows.append((s,e,span_norm))\n",
    "\n",
    "    spans = []\n",
    "    for phrase in object_phrases or []:\n",
    "        p_norm = \" \".join(normalize_tokens(tokenize(str(phrase))))\n",
    "        if not p_norm: \n",
    "            continue\n",
    "        found = False\n",
    "        for s,e,span_norm in norm_windows:\n",
    "            if span_norm == p_norm:\n",
    "                spans.append((s,e)); found = True; break\n",
    "        if not found:\n",
    "            best = None; best_r = 0.0\n",
    "            for s,e,span_norm in norm_windows:\n",
    "                r = fuzzy_ratio(span_norm, p_norm)\n",
    "                if r > best_r:\n",
    "                    best_r, best = r, (s,e)\n",
    "            if best and best_r >= 0.86:\n",
    "                spans.append(best)\n",
    "\n",
    "    spans.sort(key=lambda x: (x[1]-x[0]), reverse=True)\n",
    "    for s,e in spans:\n",
    "        if any(used[i] for i in range(s,e)): \n",
    "            continue\n",
    "        labels[s] = \"B\"\n",
    "        for i in range(s+1,e): labels[i] = \"I\"\n",
    "        for i in range(s,e): used[i] = True\n",
    "\n",
    "    return toks, labels\n",
    "\n",
    "# Print explicit BIO sequences for two examples\n",
    "for k in range(min(2, len(train_data))):\n",
    "    toks, tags = bio_label(train_data[k][\"loc_desc\"], train_data[k][\"surrounding_objects\"], max_tokens=MAX_TOKENS)\n",
    "    print(f\"Example {k}:\")\n",
    "    print(\"TOK:\", \" \".join(toks[:120]))\n",
    "    print(\"TAG:\", \" \".join(tags[:120]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c26b009",
   "metadata": {},
   "source": [
    "## 6) Build sequences & length stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99185419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(data_split: List[Dict]) -> Tuple[List[List[str]], List[List[str]]]:\n",
    "    X, Y = [], []\n",
    "    for ex in data_split:\n",
    "        toks, tags = bio_label(ex[\"loc_desc\"], ex[\"surrounding_objects\"], max_tokens=MAX_TOKENS)\n",
    "        X.append(toks); Y.append(tags)\n",
    "    return X, Y\n",
    "\n",
    "X_train, y_train = build_sequences(train_data)\n",
    "X_test,  y_test  = build_sequences(test_data)\n",
    "\n",
    "train_lens = [len(x) for x in X_train]\n",
    "test_lens  = [len(x) for x in X_test]\n",
    "print(\"Train sequences:\", len(X_train), \"| Test sequences:\", len(X_test))\n",
    "print(\"Avg train length:\", int(np.mean(train_lens)), \"| 95th pct:\", int(np.percentile(train_lens,95)))\n",
    "print(\"Avg test length:\",  int(np.mean(test_lens)),  \"| 95th pct:\", int(np.percentile(test_lens,95)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34484b1e",
   "metadata": {},
   "source": [
    "## 7) BIO tagger features (CRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642d880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_punct(tok: str) -> bool:\n",
    "    return bool(re.fullmatch(r'\\W', tok))\n",
    "\n",
    "def shape(tok: str) -> str:\n",
    "    s=[]\n",
    "    for ch in tok:\n",
    "        if ch.isupper(): s.append('X')\n",
    "        elif ch.islower(): s.append('x')\n",
    "        elif ch.isdigit(): s.append('d')\n",
    "        else: s.append(ch)\n",
    "    return \"\".join(s)\n",
    "\n",
    "PREPS = {\"in\",\"on\",\"at\",\"into\",\"inside\",\"within\",\"near\",\"by\",\"under\",\"over\",\"behind\",\"beside\",\"through\"}\n",
    "COPULAS = {\"is\",\"are\",\"was\",\"were\"}\n",
    "INTRO = {\"there\",\"here\"}\n",
    "\n",
    "def word2features(tokens, i):\n",
    "    t = tokens[i]\n",
    "    feats = {\n",
    "        'bias': 1.0,\n",
    "        'w.lower': t.lower(),\n",
    "        'w.shape': shape(t),\n",
    "        'is_title': t.istitle(),\n",
    "        'is_upper': t.isupper(),\n",
    "        'is_digit': t.isdigit(),\n",
    "        'is_punct': is_punct(t),\n",
    "    }\n",
    "    if i>0:\n",
    "        p=tokens[i-1]; feats.update({'-1.w.lower': p.lower(), '-1.is_punct': is_punct(p)})\n",
    "    else: feats['BOS']=True\n",
    "    if i<len(tokens)-1:\n",
    "        n=tokens[i+1]; feats.update({'+1.w.lower': n.lower(), '+1.is_punct': is_punct(n)})\n",
    "    else: feats['EOS']=True\n",
    "    if i>1: feats['-2.w.lower']=tokens[i-2].lower()\n",
    "    if i+2<len(tokens): feats['+2.w.lower']=tokens[i+2].lower()\n",
    "    feats['pattern_intro'] = (tokens[i-2].lower() in INTRO and tokens[i-1].lower() in COPULAS) if i>=2 else False\n",
    "    feats['prev_prep'] = tokens[i-1].lower() in PREPS if i>=1 else False\n",
    "    return feats\n",
    "\n",
    "def sequences2features(X): return [[word2features(sent, i) for i in range(len(sent))] for sent in X]\n",
    "\n",
    "Xtr_feats = sequences2features(X_train)\n",
    "Xte_feats = sequences2features(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38866e1",
   "metadata": {},
   "source": [
    "## 8) Train BIO tagger (CRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4647def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True,\n",
    "    c1=0.1, c2=0.1,\n",
    "    # Optionally adjust for imbalance:\n",
    "    # class_weight={'O': 0.05, 'B': 1.0, 'I': 0.8}\n",
    ")\n",
    "crf.fit(Xtr_feats, y_train)\n",
    "print(\"Trained. Labels:\", crf.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9150aa5c",
   "metadata": {},
   "source": [
    "## 9) Predict + show explicit BIO sequences and reconstructed objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e6dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = crf.predict(Xte_feats)\n",
    "\n",
    "def tags_to_objects(tokens: List[str], tags: List[str]) -> List[str]:\n",
    "    objs, cur = [], []\n",
    "    for tok, tg in zip(tokens, tags):\n",
    "        if tg == \"B\":\n",
    "            if cur: objs.append(\" \".join(cur)); cur=[]\n",
    "            cur=[tok]\n",
    "        elif tg == \"I\":\n",
    "            if cur: cur.append(tok)\n",
    "        else:\n",
    "            if cur: objs.append(\" \".join(cur)); cur=[]\n",
    "    if cur: objs.append(\" \".join(cur))\n",
    "    return [o.strip() for o in objs if o.strip()]\n",
    "\n",
    "for k in range(min(2, len(X_test))):\n",
    "    print(f\"Example {k}:\")\n",
    "    print(\"TOK:\", \" \".join(X_test[k][:120]))\n",
    "    print(\"TAG:\", \" \".join(y_pred[k][:120]))\n",
    "    print(\"Predicted objects:\", tags_to_objects(X_test[k], y_pred[k]))\n",
    "    print(\"Gold objects (values):\", test_data[k]['surrounding_objects'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d15487",
   "metadata": {},
   "source": [
    "## 10) Token-level evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a902bcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(crf.classes_)\n",
    "print(crf_metrics.flat_classification_report(y_test, y_pred, labels=labels, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98abe00",
   "metadata": {},
   "source": [
    "## 11) Entity-set evaluation (compare predicted spans to gold VALUE set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc809df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def norm_phrase(s: str) -> str:\n",
    "    return \" \".join(re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?|\\d+\", s.lower()))\n",
    "\n",
    "def fuzzy_hit(pred: str, golds: Set[str], thr=0.86) -> bool:\n",
    "    for g in golds:\n",
    "        if SequenceMatcher(None, norm_phrase(pred), norm_phrase(g)).ratio() >= thr:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "TP=0; Pred=0; Gold=0\n",
    "for ex, toks, tags in zip(test_data, X_test, y_pred):\n",
    "    pred_objs = set(tags_to_objects(toks, tags))\n",
    "    gold_objs = set(map(str, ex['surrounding_objects']))\n",
    "    Pred += len(pred_objs)\n",
    "    Gold += len(gold_objs)\n",
    "    for p in pred_objs:\n",
    "        if fuzzy_hit(p, gold_objs): TP += 1\n",
    "\n",
    "precision = TP/Pred if Pred else 1.0\n",
    "recall    = TP/Gold if Gold else 1.0\n",
    "f1 = 2*precision*recall/(precision+recall) if (precision+recall)>0 else 0.0\n",
    "print(f\"[Entity-set] Precision={precision:.3f} Recall={recall:.3f} F1={f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6747e1e9",
   "metadata": {},
   "source": [
    "## 12) Insights\n",
    "\n",
    "- **Model performance:**  \n",
    "  The CRF BIO tagger achieved **macro-F1 ≈ 0.98** on the test set.  \n",
    "  - `B`: Precision = **1.00**, Recall = **0.90**, F1 = **0.95**  \n",
    "  - `I`: Precision = **0.98**, Recall = **1.00**, F1 = **0.99**  \n",
    "  - Overall accuracy ≈ **0.999**. These scores show the model reliably identifies interactive objects with high precision and recall.\n",
    "\n",
    "- **BIO vs Seq2Seq:**  \n",
    "  - **BIO tagging** outputs explicit token-level labels (e.g., `O O B I O …`), making spans easy to interpret and align with the text.  \n",
    "  - **Seq2Seq** outputs a generated object list (e.g., `\"white house, path\"`), which is flexible but more computationally expensive and less transparent.  \n",
    "  - For this dataset, the BIO tagger is both efficient and effective.\n",
    "\n",
    "- **Efficiency:**  \n",
    "  - Training the CRF takes only seconds.  \n",
    "  - Inference is real-time, so it fits smoothly into a pipeline-based agent.  \n",
    "  - Seq2Seq models would require more resources to train and run.\n",
    "\n",
    "- **Error patterns:**  \n",
    "  - Occasionally misses the **beginning token** of long or unusual object phrases.  \n",
    "  - Rarely mislabels irrelevant nouns.  \n",
    "  - Continuation tokens (`I`) are handled almost perfectly.\n",
    "\n",
    "- **Improvements:**  \n",
    "  - Add POS/noun-chunk features (e.g., via spaCy) to help detect object starts.  \n",
    "  - Try neural architectures (BiLSTM+CRF, BERT+CRF) for more robust predictions.  \n",
    "  - Explore Seq2Seq approaches if canonicalized object lists are required for downstream knowledge graph construction.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Task1_WoT_Intent_Recognition.ipynb"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
